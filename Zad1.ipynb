{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentator:\n",
    "    def __init__(self, lexicon=None, filepath=None):\n",
    "        \"\"\"Initialize lexicon\"\"\"\n",
    "        if lexicon is not None:\n",
    "            self.load_lex(lexicon)\n",
    "        elif filepath is not None:\n",
    "            self.load_lex_from_file(filepath)\n",
    "        else:\n",
    "            self.lexicon = None\n",
    "    \n",
    "    def prepare_lex(self, lexicon):\n",
    "        \"\"\"Splits every whole into list of words\"\"\"\n",
    "        for i in range(len(lexicon)):\n",
    "            lexicon[i] = np.array(lexicon[i].split())\n",
    "        self.lexicon = lexicon\n",
    "        \n",
    "    def load_lex_from_file(self, filepath):\n",
    "        \"\"\"Loads lexicon from file \"lexicon.txt\".\n",
    "        Every whole is store as list of words.        \n",
    "        \"\"\"\n",
    "        self.lexicon = []\n",
    "        with open(\"lexicon.txt\", \"r\") as file: \n",
    "            for line in file: \n",
    "                line = line.split()\n",
    "                self.lexicon.append(line)\n",
    "        \n",
    "    def segment(self, text):\n",
    "        \"\"\"Segment text.\n",
    "        Punctuactions (others than dash between words)\n",
    "        are treated as separetly segments.\n",
    "        Words that create whole described \n",
    "        in \"lexicon.txt\" are theated as \n",
    "        total segment.\n",
    "        \n",
    "        Parameters:\n",
    "        text str: analyzed text\n",
    "\n",
    "        Returns:\n",
    "        segments List[str]: segmented text\n",
    "        \"\"\"\n",
    "        text = self._removeNewlines(text)\n",
    "        segments = text.split()\n",
    "        self._split_punctuation(segments)\n",
    "        if self.lexicon is not None:\n",
    "            self._merge_wholes(segments)\n",
    "        self._add_eos(segments)\n",
    "        return segments\n",
    "        \n",
    "    def _removeNewlines(self, text):\n",
    "        \"\"\"Remove newlines from text and save breaked words\"\"\"\n",
    "        return text.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "    \n",
    "    def _split_punctuation(self, segments):\n",
    "        \"\"\"Split punctions that appear at the end of segments\"\"\"\n",
    "        for i in reversed(range(len(segments))):\n",
    "            if segments[i][-1] in string.punctuation:\n",
    "                segments.insert(i+1, segments[i][-1])\n",
    "                segments[i] = segments[i][:-1]\n",
    "                \n",
    "    def _add_eos(self, segments):\n",
    "        \"\"\"Add <eos> at the end of a sentence\"\"\"\n",
    "        segments.append(\"<eos>\")\n",
    "        for i in reversed(range(len(segments)-1)):\n",
    "            if segments[i][-1] in [\".\", \"!\", \"?\"] and segments[i+1][0].isupper():\n",
    "                segments.insert(i+1, \"<eos>\")\n",
    "    \n",
    "    def _merge_wholes(self, segments):\n",
    "        \"\"\"Merge segments that form a whole\n",
    "        \n",
    "        Parameters:\n",
    "        segments List[str]: segmented text\n",
    "        \"\"\"\n",
    "        segments_to_merge = self._get_segments_to_merge(segments)\n",
    "        for seg in segments_to_merge:\n",
    "            start, end, whole = seg.values()\n",
    "            segments[start] = whole\n",
    "            del segments[start+1:end]\n",
    "    \n",
    "    def _get_segments_to_merge(self, segments):\n",
    "        \"\"\"Get wholes from text.\n",
    "        \n",
    "        Parameters:\n",
    "        segments List[str]: segmented text\n",
    "\n",
    "        Returns:\n",
    "        segments_to_merge List[int, int, str]: \n",
    "            matrix n x 3 with\n",
    "            every row containing:\n",
    "            1. whole appearance beginning\n",
    "            2. whole appearance ending\n",
    "            3. whole sentence\n",
    "        \"\"\"\n",
    "        segments_to_merge = []\n",
    "        indices, wholes = self._get_potential_wholes_beginning_indicies(segments)\n",
    "        for start, whole in zip(indices, wholes):\n",
    "            end = start + len(whole)\n",
    "            if (end < len(segments)) and (segments[start:end] == whole):\n",
    "                segments_to_merge.append({\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"whole\": \" \".join(whole)\n",
    "                })\n",
    "        return segments_to_merge\n",
    "    \n",
    "    def _get_potential_wholes_beginning_indicies(self, segments):\n",
    "        \"\"\"Give indicies where can possibly start wholes in text\n",
    "        \n",
    "        Parameters:\n",
    "        segments List[str]: segmented text\n",
    "\n",
    "        Returns:\n",
    "        indicies List[int]: indicies where can start potential wholes\n",
    "        wholes List[List[str]]: potential wholes (words splited into list)\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        wholes = []\n",
    "        for whole in self.lexicon:\n",
    "            for i, word in enumerate(segments):\n",
    "                if word == whole[0]:\n",
    "                    indices.append(i)\n",
    "                    wholes.append(whole)\n",
    "        return indices, wholes\n",
    "    \n",
    "    def print_segments(self, segments):\n",
    "        \"\"\"Prints segments\"\"\"\n",
    "        for s in segments:\n",
    "            print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piłka nożna to wspaniały sport.\n",
      "Biało-czerwoni zajeli swoje pozycje.\n",
      "Żółta kartka dla napastika należy się.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = None\n",
    "with open(\"text.txt\") as file:\n",
    "    text = file.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piłka nożna\n",
      "to\n",
      "wspaniały\n",
      "sport\n",
      ".\n",
      "<eos>\n",
      "Biało-czerwoni\n",
      "zajeli\n",
      "swoje\n",
      "pozycje\n",
      ".\n",
      "<eos>\n",
      "Żółta kartka\n",
      "dla\n",
      "napastika\n",
      "należy\n",
      "się\n",
      "należy się\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "segmentator = Segmentator(filepath=\"lexicon.txt\")\n",
    "segments = segmentator.segment(text)\n",
    "segmentator.print_segments(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co można jeszcze:\n",
    "# inż, czy łączyć myślniki czy rozdzielać, daty; podział na strukture dokumentu (akapity, nagłówki)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
